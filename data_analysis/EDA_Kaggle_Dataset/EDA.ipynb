{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# EDA (Exploratory data analysis)\n",
    "## For \"Fake News\" dataset\n",
    "### Developed by: Sebastián Marroquín\n",
    "\n",
    "***\n",
    "\n",
    "### Tasks to do\n",
    "\n",
    "1. Import the necessary libraries\n",
    "2. Load the dataset\n",
    "    - Verify if the data contained in the data set are necessary to perform an analysis, if not, correct them.\n",
    "3. Obtain and visualize the different types of news contained within the data set\n",
    "    - Graph these types of news\n",
    "4. WordCloud\n",
    "5. Dataset visualization\n",
    "\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. Import libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Import the libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "sns.set(style=\"darkgrid\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### 2. Load the dataset's"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realDf = pd.read_csv('True.csv')\n",
    "fakeDf = pd.read_csv('Fake.csv')"
   ]
  },
  {
   "source": [
    "#### Head of Real & Fake New's"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeDf.head()"
   ]
  },
  {
   "source": [
    "#### Info about the dataframe's"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeDf.info()"
   ]
  },
  {
   "source": [
    "#### We will add the target value that each data set represents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realDf['target'] = 1\n",
    "fakeDf['target'] = 0"
   ]
  },
  {
   "source": [
    "#### Show the dataset's tail"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realDf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeDf.tail()"
   ]
  },
  {
   "source": [
    "### 3. WordCloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Fake news titles in WordCloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = fakeDf.title\n",
    "wordcloud = WordCloud(\n",
    "    width = 2000,\n",
    "    height = 1000,\n",
    "    background_color = 'black',\n",
    "    stopwords = STOPWORDS).generate(str(text))\n",
    "fig = plt.figure(\n",
    "    figsize = (30, 20),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Fake News Titles')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "#### Real news in WordCloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = realDf.title\n",
    "wordcloud = WordCloud(\n",
    "    width = 2000,\n",
    "    height = 1000,\n",
    "    background_color = 'black',\n",
    "    stopwords = STOPWORDS).generate(str(text))\n",
    "fig = plt.figure(\n",
    "    figsize = (30, 20),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Real News Titles')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We will combine the data sets in a single variable, this in order to have the target variables of each of the news within the data sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData = pd.concat([realDf, fakeDf], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tail of the merged data\n",
    "mergeData.tail()"
   ]
  },
  {
   "source": [
    "### 3. Data visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### News distribution by objective variables of false or real"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = sns.countplot(x='target', data=mergeData, palette='Set2')\n",
    "axis.set(xticklabels = ['fake', 'real'])\n",
    "plt.title(\"Data distribution of Fake & Real New's\")"
   ]
  },
  {
   "source": [
    "#### News distribution by type of news"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "axis = sns.countplot(x='subject', hue='target' ,data=mergeData, palette='Set2')\n",
    "plt.title(\"Data distribution of Fake & Real New's by Subject\")"
   ]
  },
  {
   "source": [
    "#### Count for subject"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeData.subject.value_counts()"
   ]
  },
  {
   "source": [
    "### 4. Data cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_data(x):\n",
    "    text = x\n",
    "    text = text.lower()\n",
    "    # remove square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text) \n",
    "    # remove word's containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanMergeData = mergeData.copy()\n",
    "cleanMergeData['text'] = mergeData.text.apply(lambda x : clean_train_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanMergeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanMergeData.tail()"
   ]
  },
  {
   "source": [
    "### 5. Remove the stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enStopWords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_eng_stopwords(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [word for word in token_text if word not in enStopWords]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordMergeData = cleanMergeData.copy()\n",
    "stopWordMergeData['text'] = cleanMergeData.text.apply(lambda x : remove_eng_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordMergeData.head()"
   ]
  },
  {
   "source": [
    "### 6. Most common words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}